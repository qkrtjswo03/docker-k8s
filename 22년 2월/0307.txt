가상머신  일시중지  중이라면  
실행한  뒤  재부팅[가상머신]  한번  해주세요
4대  모두  재부팅이  끝났으면  manager 에서

docker node ls 해서  상태  확인해  주세요

docker  service  ls 한 다음 동작중인 서비스가 있다면 
docker  service  rm  "서비스명"


docker  container  run  -> 컨테이너 배포 1개만 생성
기본) 도커 허브-> 이미지 pull  -> 로컬 이미지 저장소에 저장 -> 도커가 이미지를 확인하고 해당 이미지를 이용하여 컨테이너를 배포한다. 
DB--WEB-> 80 export

DB 컨테이너 배포 -> WEB 컨테이너 배포(DB와 link,, port  를 외부에 노출)
삭제?? 동일 환경을 또 만들어야 한다면??

이를 해결하기 위해 템플릿 파일 작성 yml

yml  파일내에 필요한 환경을 구성하고 이를 docker-compose  를 이용하여 배포한다.  
docker-compose  는 도커엔진에 기본적으로 포함되어 있는 프로젝트가 아니므로 별도의 설치과정이 필요하다.  

compose  를 이용하면 환경배포, 환경 삭제를 한번에 진행할 수 있다. 
yml  파일이 있는 디렉토리에서 docker-compose  down  을 하면 yml  파일을 읽고 해당 환경을 한번에 지울 수 있게된다. 재사용이 가능하다. 

지금까지의 모든 설정은 서버 1대 위에서 동작하는 것이다. 


여러 서버에 동일 컨테이너(서비스)를 배포할 수 있어야 한다. 
모든 컨테이너는 동일 네트워크에서 연결되어야 한다. 물리적으로는 불가능하지만 논리적으로 이를 연결할 수 있어야 한다. 또한 각 서버에 위치한 물리자원들(CPU,,RAM)) 을 통합관리할 수 있어야 한다. 이를 위해서는 "cluster"" 와 클러스터 환경에서 컨테이너를 오케스트레이션 할 수 있어야 한다. -> 컨테이너 오케스트레이션 툴 (docker  swarm,, kubernetes))


스웜 클래식 vs  스웜 모드(일반적으로 도커 스웜을 얘기한다)

manager  는 자신도 자신에 대한 agent  와 runtime((docker))을 가지고 있으므로 업무가 지시되면 manager  도 업무 처리에 참여한다. 결국 manager  가 서비스 생성을 명령하면 클러스터에 있는 모든 노드(manager,worker)) 는 이를 전달받고 처리한다. 

 매니저와 워커는 누가 토큰을 발급하고 이를  전달하는지에 달려있다. 

[manager]
docker  swarm  init  --advertise-addr=211.183.3.100
-> 토큰이 발행된다. (워커 가입용 토큰)

user1@manager:~$ docker swarm join-token
manager  worker
user1@manager:~$ 

마지막에 manager  를 입력해 보면 추가매니저를 위한 토큰이 발행되어 있는 것을 확인할 수 있다. 

안정적인 클러스터 운영을 위해서는 manager  를 최소한 2대 운영해야 한다. 

클러스터 환경내에서 동작중이던 manager  가 없어진다면 해당 클러스터를 관리할 수 없게된다. 

만약 동작중인 worker  중에 한 대를 manager  로 운영하고 싶다면?
docker  node  promote  worker1  <-- worker  => manager
docker  node  demote  worker1 <-- worker  <= manager


매니저가 가입되어 있는 node  를 클러스터에서 제외시키고 싶다면??
docker  node  rm  "ID"  <--- 해당 노드의 상태가 Down  상태
docker  node  rm  --force  "ID"    <-- Active  상태에서도 강제로 제외 시킬 수 있다

만약 스스로 클러스터에서 떠나고 싶다면??
docker  swarm  leave  [--force]


우리가 관리하는 서버가 1000 대다!!!
이미지가 현재 없는 상태이다. 이 상태에서 docker-hub 에 있거나 또는 사설 저장소에 있는 이미지를 이용하여 서비스를 배포하고 싶다면???

각 노드에 이미지를 다운로드 해 두어야 한다.
1. 스크립트를 이용하거나 ansible/terraform  등을 이용하여 전체 서버에 docker  pull  centos:7 명령을 미리 전달해 둔다. 

2. manager  에서 명령을 노드에 전달할 때(이미지 pull  을 전달하는 것이 아니라 서비스 배포해라)  일반적인 경우라면 pull  속도에 따라 빨리 pull  된 곳에서만 컨테이너가 배포되는 불균형이 발생할 수 있다. 

1,2 모두다 각자 docker  login  을 해야 한다. 1 은 스크립트 또는 ansible  등을 통해 한번의 명령으로 모두다 로그인 할 수 있다.
하지만 2번은 불가능하다. 

이 경우 전에 워커에서 로그인 하는 것은 어려움이 있을 수 있으므로 다음과 같은 옵션을 사용할 수 있다.  "--with-registry-auth" 이 경우 manager  에서 사용한 로그인 정보를 통해 워커들이 별도의 인증 작업없이 이미지 다운->서비스 배포가 가능해 진다. 


user1@manager:~$ docker network ls
NETWORK ID     NAME              DRIVER    SCOPE

874465a19da7   docker_gwbridge   bridge    local

bkat1tyr50le   ingress           overlay   swarm
user1@manager:~$

ingress  는 생성된 서비스(컨테이너)가 오버레이 네트워크를 통해 연결된다. 즉, 생성된 서비스들이 자동으로 연결되는 네트워크이며 외부와의 통신에 참여하는 것이 아니라 내부로 접속을 요청하는 경우 RR 을 통해 연결을 수행한다.

docker__gwbridge  는 ingress  네트워크에 포함된 서비스들이 외부와 통신할 때 사용하는 bridge  .

따라서 docker  container  run  을 통해 생성된 컨테이너들과 swarm  클러스터 상에 배치된 컨테이너는 서로 다른 네트워크에 배치되고 직접 통신 되지 않는다. 

또한 기본적으로 docker  container  run  으로 생성된 컨테이너들은 ingress  네트워크에 연결될 수 없다. 



서비스  배포시  : 
--replicas 4 는  명시적으로  필요한  컨테이너  개수를  지정하는  방식
docker service scale testweb=2 와  같은  방식으로  스케일  조정이  가능하다.. 

--mode global :  swarm 클러스터에  참여하는  모든  노드에  각각  1 개의  컨테이너  배포
[scale 조정은  지원되지  않는다]]

롤링업데이트
기존  컨테이너가  포함하고  있는  내용을  동작중인  상태에서  변경하는  것이  아니다..
실제로는  기존  컨테이너가  중지되고  새로운  버전으로  생성된  컨테이너가  이  역할을  대체하는  것이다.. 

만약  기존  컨테이너로  돌아가고  싶다면  이를  롤백  할  수  있다..!!!

저장소가  필요하다!!!! 
manager 에  사설  저장소를  만들고  여기에  이미지를  push 하여  사용해  본다..

도커는  docker hub, 사설저장소로  접속하여  이미지를  pull/push 할  때  인증정보를  포함하고  있는지,, 해당  인증정보가  보안성을  갖추고  있는지를  확인하고  만약  해당  정보가  없다면  접속을  자동으로  차단시킨다.. 

 docker pull centos:7  <--- docker-hub
 docker pull user1/centos:7 <--- docker-hub
 docker pull 1.1.1.1:5000/mycentos:1.0   <--- private registry 

실습의  편의를  위해  오늘은!!! 인증정보없이  push, pull 을  하더라도  도커가  이를  허용하도록  설정해  본다.. 


docker container run -d --name registry  -p 5000:5000 --restart=always registry

docker run -d -p 8080:8080 --name registry-web --restart=always --link registry -e REGISTRY_URL=http://registry:5000/v2 -e REGISTRY_NAME=211.183.3.100:5000 hyper/docker-registry-web

vi /etc/docker/daemon.json 파일을  만들고 아래  내용  입력
[worker1 ~ worker3]
{
        "insecure-registries":["211.183.3.100:5000"]
}
docker pull 211.183.3.100:5000/mycentos:1.0
[manager 에서는  아래와  같이입력  ]
{
        "insecure-registries":["localhost:5000"]
}

만약  위처럼  입력했다면,, manager 에서  해당  이미지  pull 하고  싶다면

docker pull localhost:5000/mycentos:1.0

user1@worker1:~$ sudo  systemctl  restart  docker
user1@worker1:~$
user1@worker1:~$ docker  push  211.183.3.100:5000/mycentos:1.0
The push refers to repository [211.183.3.100:5000/mycentos]
174f56854903: Pushing  95.99MB/203.9MB

이제  각  노드에서  이미지를  pull 해  본다..
manager  -> docker pull localhost:5000/mycentos:1.0
wokers      -> docker pull 211.183.3.100:5000/mycentos:1.0

blue/Dockerfile
blue/index.html
--> 211.183.3.100:5000/myhttpd:blue

user1@manager:~/cimg/blue$ docker build --tag myhttpd:blue .
user1@manager:~/cimg/blue$ docker tag myhttpd:blue localhost:5000/myhttpd
user1@manager:~/cimg/blue$ docker push localhost/myhttpd:blue

green/Dockerfile
green/index.html
--> 211.183.3.100:5000/myhttpd:green 

user1@manager:~/cimg/blue$ docker build --tag myhttpd:green .
user1@manager:~/cimg/blue$ docker tag myhttpd:green localhost:5000/myhttpd
user1@manager:~/cimg/blue$ docker push localhost/myhttpd:green

작성된 두 이미지는 사설 저장소 (211.183.3.100:5000) 에 push 해두세요





user1@manager:~/cimg/blue$ docker service create -d -p 80:80 --name  myhttpd --replicas 3 211.183.3.100:5000/myhttpd:blue

서비스  배포시  manager 에서는  이미지이름을  localhost:5000/myhttpd:blue 로  해  둔  상태이다. 매니저가  211.183.3.100:5000 으로  접속하여  이미지를  pull 하려고  하더라도  daemon.json 에서는  insecure 를 localhost:5000 에  대해서만  인증을  적용하지  않겠다고  했으므로  해당  이미지를  pull 할  수  없다.. 따라서  Round Robin 에  의해  worker1~worker3 에서  서비스가  배포된다.. 
하지만  211.183.3.100으로  웹  접속하더라도  오버레이를  통해  타  호스트에  있는  컨테이너로  부터 웹 서비스를  제공받을  수  있게된다..


user1@manager:~$ docker service  update  --image 211.183.3.100:5000/myhttpd:green  myhttpd

myhttpd 서비스에  대하여  동작중인 상태에서  기존  이미지에서  새로운  이미지  green 으로  업데이트  하여  서비스를  제공한다.. 

user1@manager:~$ docker  service  inspect myhttpd --pretty | grep  Image

 Image:         211.183.3.100:5000/myhttpd:green
user1@manager:~$
user1@manager:~$ docker  service  rollback myhttpd
user1@manager:~$
user1@manager:~$ docker  service  inspect myhttpd --pretty | grep  Image
 Image:         211.183.3.100:5000/myhttpd:blue
user1@manager:~$


오후에는  컨테이너에  특정  변수,, 파일내용,, 비밀번호  등을  전송할  수  있는  config,secret
서비스  배포시  특정  노드,, role, label 에서만  서비스가  배포되도록  하기  위한  labeling 을  
도커  스웜  네트워크

내일  도커스웜  + compose(yml) -> 도커  스택


Secret/Config
둘다 컨테이너로 특정 정보를 전달하기 위한 목적으로 사용한다. 
컨테이너 내에서 사용이 되더라도 만약 컨테이너가 삭제되면 해당 secret, config 는 휘발성 이므로 없어진다. 


이미지는 정적인 결과물이므로 내용을 수정할 수는 없다.
이미지내에 패스워드, 변수, key 값등이 적용되어 있다면 이를 이용하여 생성한 모든 컨테이너는 동일한 pwd, 변수, key 값을 사용할 수 밖에 없게된다. 이는 보안상 결함이라고 할 수 있다.

이 경우 이미지에 Secret/Config 를 사용하면 필요할 때마다 value 를 변경하여 컨테이너를 배포하는 형식으로 각 컨테이너 별로 별개의 key,password, 변수 등을 전달할 수 있게된다.

Secret, Config 는 docker container run 으로는 사용할 수 없다. 
오직 docker swarm  에서만 사용할 수 있다. 만약 docker container 에서 패스워드를 적용하고자 한다면 -e PASS=test123 과 같이 보안성이 없는 상태에서 데이터를 저장할 수밖에 없다.

Secret 는 컨테이너 내에서는 평문, 컨테이너 삭제시 함께 사라진다.(휘발성)

Secret -> 패스워드, ssh key, 인증서와 같이 중요한 데이터를 전송할 때 사용한다.
Config -> 설정파일, 변수와 같이 암호화가 필요하지 않은 경우에 사용한다..


사용하는 방법
1. secret/config 를 생성한다.
2. 서비스 생성시 "1" 에서 만든 결과물을 컨테이너의 어디에서 사용할 것인지 지정해 준다.

Secret 생성하기
user1@manager:~$ echo test123 | docker secret create mypwd -
user1@manager:~$ docker secret create mypwd2 pwd.txt

 docker service create --name mysql --replicas 1 \
> --secret mypwd -e MYSQL_ROOT_PASSWORD_FILE="/run/secrets/mypwd"

"나는 mysql 이라는 서비스를 1개 배포할 예정인데.. mysql 내에서 사용할 root 의 패스워드는 컨테이너 내에 있는 /run/secrets/mypwd 파일의 내용을 토대로 설정하겠다.
--secret mypwd 를 적용하게 되면 mypwd 의 내용을 컨테이너 내의 /run/secrets/mypwd 에 기본적으로 붙여넣기 해 준다."

user1@manager:~$ docker service create --name mysql --replicas 1 \
> --secret mypwd -e MYSQL_ROOT_PASSWORD_FILE="/run/secrets/mypwd" \
> -e MYSQL_PASSWORD_FILE="/run/secrets/mypwd" \
> -e MYSQL_DATABASE="wordpress" mysql:5.7

user1@manager:~$ docker service ps mysql
ID             NAME      IMAGE       NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS
wk41dda6jzmp   mysql.1   mysql:5.7   worker3   Running         Running 27 seconds ago
user1@manager:~$  # 위의 경우에는 worker3 에 서비스가 배포되었다.
user1@worker3:~$ docker container ls
CONTAINER ID   IMAGE       COMMAND                  CREATED              STATUS              PORTS                 NAMES
637b13f04e14   mysql:5.7   "docker-entrypoint.s…"   About a minute ago   Up About a minute   3306/tcp, 33060/tcp   mysql.1.wk41dda6jzmp11rkrt89sc4a7
user1@worker3:~$ docker container exec mysql.1.wk41dda6jzmp11rkrt89sc4a7 cat /run/secrets/mypwd
test123
user1@worker3:~$
위와 같이 컨테이너 내에서는 평문으로 전달 된 것을 확인할 수 있다. 



 관리자 한명이 전체 애플리케이션을 동일하게 배포한다면 secret 을 사용하지 않고 패스워드를 직접 입력해도 상관없을 것이다. 하지만, 고객이 우리 클라우드 환경에서 컨테이너를 통해 애플리케이션을 배포하고자 한다면 고객이 입력한 패스워드가 외부에 노출이 되어서는 절~대 안된다. 

고객이 클라우드 환경에 들어와서 원하는 패스워드를 입력하면 해당 패스워드는 Secret 으로 저장하여 외부 타 사용자, 클라우드 관리자 모두 볼 수 없도록 해야한다. 

  희망하는 DB  패스워드 :  ________   -> Secret -> gildong 이라는 secret 이 생성
 
단, 일반적인 설정 파일의 내용이나 간단한 변수등은 보안이 필요하지 않으므로 이는 config 를이용하여 배포한다.


[Config]
사용자가 index.html 파일을 올리면 -> config -> /var/www/html/index.html 아래에 전달

echo "<h2>HELLO</h2>" > index.html
docker config create webconfig index.html
docker service create --replicas 1 -d \
> -p 8001:80 --name webconfig1 --config source=webconfig,target=/usr/share/nginx/html/index.html nginx

user1@manager:~/0307$ docker service ps webconfig1
ID             NAME           IMAGE          NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS
iyreblp4mj5g   webconfig1.1   nginx:latest   worker2   Running         Running 47 seconds ago
user1@manager:~/0307$ 위처럼 worker2 에서 동작중이라는 것을 확인할 수 있다.
worker2  에 가서
docker container exec "컨테이너이름" cat /usr/share/nginx/html/index.html 
내용을 확인해 보면 config 만들때 사용했던 index.html 의 내용을 확인할 수 있을 것이다.

외부에서는 http://211.183.3.100:8001 로 접속해 보면 설정된 index.html config 내용을 확인할 수 있다.

user1@manager:~/0307$ curl http://211.183.3.102:8001
<h2>HELLO</h2>
user1@manager:~/0307$

 

도커 네트워크
ingreess : 로드밸런스

기본 ingress 가 아니라 사용자가 만든 overlay  네트워크를 이용하여 서비스를 배포해 본다.

user1@manager:~/0307$ docker network create -d overlay \
> --subnet 192.168.111.0/24 dildong

user1@manager:~/0307$ docker network ls
NETWORK ID     NAME              DRIVER    SCOPE
f1dck9esnndy   dildong           overlay   swarm

위와 같이 생성된 gildong 네트워크는 전체 호스트를 가로지르는 오버레이 네트워크이며 SCOPE가 swarm 이므로 스웜 클러스터에 참여하는 서비스들이 연결될 수 있는 네트워크이다. 그렇다면 서비스를 이용하여 만든 것(컨테이너)이 아니라 docker container run 을 이용하여 만든 컨테이너는 gildong 네트워크에 연결이 가능한가???  --net gildong 가능?? 불가능하다.

만약 docker container run 을 이용하여 생성한 단독 컨테이너를 스웜 클러스터의 overlay 네트워크에 연결하고 싶다면 overlay 네트워크 생성시 --attachable 을 붙여서 생성해야 한다.

user1@manager:~/0307$ docker network create --subnet 192.168.112.0/24 -d overlay --attachable gildong2
user1@manager:~/0307$ docker network ls | grep gildong2
ox24ws32rp8x   gildong2          overlay   swarm
user1@manager:~/0307$
user1@manager:~/0307$ docker container run -it \
> --net gildong2 centos:7
user1@manager:~/0307$ docker container inspect 771141f450de | grep IPA
            "SecondaryIPAddresses": null,
            "IPAddress": "",
                    "IPAMConfig": {
                    "IPAddress": "192.168.112.2",   <-- gildong2 네트워크에 연결됨
user1@manager:~/0307$


[볼륨]
일반적으로 도커에서 사용하는 두가지 볼륨형태를 스웜에서도 사용할 수 있다.
1. file storage      -> -v (또는 -- volume) 
          -v /root:/root -> 호스트의 /root 와   컨테이너의 /root 를 마운트 하는 방법
           디렉토리와 디렉토리를 연결하는 방법        

2. block storage  -> 
           -v myvolume:/root -> 

우와 같은 방법을 스웜에서도 사용할 수 있다.
user1@manager:~$ docker service create -d --name testvol1 \
> --mount type=volume,source=vol1,target=/root centos:7

스웜 클러스터 내의 임의의 호스트에 vol1   이라는 볼륨이 생성되고 해당 볼륨을  testvol1 이라는 서비스에 연결하여 사용한다. 해당 서비스는 자신의 /root 디렉토리가 vol1 이다. 

source=vol1 을 사용하지 않으면 임의의 ID 를 발행하고 해당 볼륨을  타겟 서비스의 /root 와 연결해 준다.

위의 방법은 block storage 를 연결하는 방법이다. 이제 nfs 처럼 호스트의 특정 디렉토리와 마운트 하는 방법을 활용해 본다.

user1@manager:~$ docker service create --name testvol2 \
> -d --mount type=bind,source=/root/host,target=/root/container \
> centos:7

임의의 호스트의 /root/host 디렉토리와 컨테이너의 /root/container 디렉토리를 마운트 하는 방법으로 볼륨을 제공하는 형식

하지만, 위의 방법처럼 컨테이너 또는 서비스에게 볼륨을 제공하지는 않는다. 

위에서 살펴본 스토리지는 컨테이너들과 아래의 호스트간 연결을 기본으로 하므로
만약 컨테이너 내의 특정 디렉토리와 호스트의 디렉토리를 연결했을 때 전체 호스트에 있는 컨테이너의 데이터를 동기화 하고 싶다면 모든 파일을 일일이 수정해야 하는 번거러움이 있다.

이를 위해 스웜 클러스터 외부에 별도로 스토리지를 준비하고 각 컨테이너는 이 스토리지에 연결하여 디스크 또는 디렉토리를 사용하는 방법을 적용해야 안정적으로 스토리지 제공이 가능하다. -> Persistent Storage(영구 스토리지)

이는 호스트, 컨테이너와 별개로 외부에 존재하는 스토리지 이며 네트워크를 통해 마운트 시켜 사용할 수 있다. 

PV(Persistent Volume), PVC(Persistent Volume Claim)
개발자가 자신이 원하는 크기의 디스크 사이즈, 사용용도(읽기/쓰기, 동시 여러명 접속 여부) 등을 지정하여 볼륨에게 요청하면 해당 요청에 적절한 볼륨을 개발자 서버에게 자동으로 연결해 주는 기술.

[라벨 사용하기] 쉽다!!!
지금까지의 서비스 배포는 replicas 또는 global 을 이용하여 특정 개수만큼 또는 각 호스트 별로 1개씩 서비스를 배포하는 방식이었다. 이로인해 manager 에서도 서비스가 배포되는 일이 발생하는데, 만약 서비스 배포를 특정 노드에서만 배포하고 싶다면??? 


node.role
node.hostname
node.id


                worker1          worker2         worker2
 zone     seoul                 seoul                 busan
 srv          web                   nfs                     web


user1@manager:~$ docker node update --label-add zone=seoul worker1
worker1
                                            docker node update --label-rm zone worker1

user1@manager:~$ docker node update --label-add zone=seoul worker2
worker2
user1@manager:~$ docker node update --label-add zone=busan worker3
worker3

user1@manager:~$ docker service create -d --name webtest --constraint 'node.labels.zone == seoul' -p 80:80 --mode global nginx
xxs3lh9oxgswjz69z39im7k64
user1@manager:~$ docker service ps webtest
ID             NAME                                IMAGE          NODE      DESIRED STATE   CURRENT STATE           ERROR     PORTS
u2qtop1scigk   webtest.93b6u5n7vpyraf2lkqho9og16   nginx:latest   worker2   Running         Running 8 seconds ago
uxaho3dg2ez3   webtest.v0vl8q6bp2fk7uqmlu70iomzw   nginx:latest   worker1   Running         Running 8 seconds ago
user1@manager:~$

Quiz. 
                worker1          worker2         worker2
 zone     seoul                 seoul                 busan
 srv          web                   nfs                     web

위의 표를 참고하여 각 노드에 라벨을 부착하고 최종적으로
서울에 있는 웹서버에만 nginx 를 배포하라!!!!

만약 라벨을 삭제하고 싶다면
docker node update --label-rm 라벨키 호스트명


Quiz. 
우리회사는 MSP 업체이다. 우리회사가 관리하는 업체중에 samsung 이 있다.
삼성의 홈페이지는 https://www.samsung.com/sec/ 이다. 

위 홈페이지 내용을 curl  https://www.samsung.com/sec/ > samsungv1.html
파일의 title 을 수정하여 samsungv2.html 로 수정하라. 

우리회사에서 삼성 홈페이지는 worker1, worker2 에서 관리하고 있다. 

우리는 삼성 페이지를 centos:7 의 /var/www/html/ 아래에 samsungv1.html 로 첫번째 이미지(211.183.3.100:5000/samsung:1.0)를 만들고 samsungv2.html 로 두번째 이미지(211.183.3.100:5000/samsung:2.0)를 생성하여 사설 저장소에 미리 업로드 해 둔다.

먼저 첫번째 이미지를 이용하여 worker1, worker2 에 배포하고 외부에서 접속 상태를 확인한다. 이후 업데이트를 통해 타이틀이 변경되었는지를 확인한다. 

단, 삼성을 위한 오버레이 네트워크는 samsung 으로 만들고 주소 대역은 10.10.3.0/24 로 할당한다. 

